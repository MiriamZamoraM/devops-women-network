# DevOps Women Network

Plataforma diseÃ±ada con las siguientes herramientas:
- Django REST Framework
- Docker
- Kubernetes
- AWS
- Terraform
- CI/CD con GitHub Actions

## ğŸ“˜ Proyecto: DevOps Women Network â€” Kubernetes Learning Journey

Este repositorio documenta mi camino de aprendizaje en DevOps, creando paso a paso un sistema desplegado en Kubernetes. Cada dÃ­a incorpora un nuevo concepto y una mejora al proyecto.

---

## ğŸŸ£ Progreso actual (DÃ­a 1â€“5)

Hasta este punto he logrado:

### âœ” 1. Deployment funcional

* Usa imagen `nginx:latest`
* Incluye variables de entorno desde Secrets
* Monta un ConfigMap como volumen en `/usr/share/nginx/html`

### âœ” 2. ConfigMap operativo

Se usa para servir contenido HTML personalizado dentro de Nginx.

### âœ” 3. Secret cifrado en base64

Almacena una variable sensible (`DB_PASSWORD`) que se inyecta en el Pod.

### âœ” 4. Service tipo ClusterIP

Provee networking interno entre Pods.

### âœ” 5. DNS interno funcionando

Validado mediante un Pod temporal (`test-pod`) con `curl` instalado.

Tuve que:

* reinstalar curl dentro del test-pod
* verificar endpoints del Service
* corregir un `ImagePullBackOff`
* validar coredns en running state

### âœ” 6. Ingress configurado y funcionando

* Se creÃ³ un archivo ingress.yaml con las reglas de enrutamiento.
* Traefik detectÃ³ y aplicÃ³ la regla para el host localhost.
* El trÃ¡fico HTTP externo entra al clÃºster y llega al Service hello-service.

DepuraciÃ³n avanzada

* Durante este dÃ­a se resolvieron problemas relacionados con:
* ImagePullBackOff por fallas de red.
* Reinicio de CoreDNS.
* Re-creaciÃ³n de Pods para tomar imÃ¡genes locales.
* ValidaciÃ³n del funcionament del Ingress con Traefik.


Â¡Ingress funcionando y trÃ¡fico HTTP fluyendo correctamente! ğŸ‰

### âœ” 7. ReconstrucciÃ³n completa del clÃºster

* Durante este dÃ­a el clÃºster presentÃ³ fallas crÃ­ticas:
* ImagePullBackOff incluso para imÃ¡genes pequeÃ±as
* Pod de pruebas (dns-check) tambiÃ©n fallaba
* Fallas de DNS interno
* Traefik no podÃ­a enrutar al Service porque no existÃ­an endpoints saludables


La causa: k3d perdiÃ³ conectividad externa y DNS interno se corrompiÃ³, por lo cual el cluster no podÃ­a descargar imÃ¡genes ni resolver dominios.

#### SoluciÃ³n implementada

Se recreÃ³ el clÃºster desde cero:

- k3d cluster delete dev
- k3d cluster create dev --servers 1 --agents 0 --port "80:80@loadbalancer"


DespuÃ©s se verificÃ³ conectividad con:

- kubectl run dns-check --image=alpine --restart=Never -- sh -c "apk add bind-tools >/dev/null && nslookup google.com"

Una vez confirmado el acceso a Internet, se aplicÃ³ nuevamente toda la infraestructura


####  CÃ³mo desplegar todo hasta ahora
```
kubectl apply -f infra/k8s/configmap-html.yaml
kubectl apply -f infra/k8s/secret.yaml
kubectl apply -f infra/k8s/deployment.yaml
kubectl apply -f infra/k8s/service.yaml
kubectl apply -f infra/k8s/service-clusterip.yaml
kubectl apply -f infra/k8s/ingress.yaml
```
---
Ver Pods:

```
kubectl get pods
```
---
Ver Service:

```
kubectl get svc
```
---
Probar DNS interno con test-pod:

```
kubectl run test-pod --image=alpine -- sleep 999999
kubectl exec -it test-pod -- sh
apk add curl
curl hello-service
```
---
Resultado

* Pod del Deployment en estado Running
* DNS interno de Kubernetes funcionando
* CoreDNS operativo
* Ingress de Traefik nuevamente accesible en http://localhost
* Endpoints del Service correctos y en estado Ready

---

## ğŸ“‚ Estructura del proyecto

```
infra/
  k8s/
    configmap-html.yaml
    deployment.yaml
    secret.yaml
    service.yaml
    service-clusterip.yaml
    ingress.yaml
```
---

## ğŸ”® Siguiente paso: DÃ­a 8

Avanzar en este proyecto


---

<p align="center">
  <strong>ğŸ€ Miriam Zamora Â· Backend & DevOps in Progress ğŸ€</strong><br/>
  <em>Code. Ship. Repeat.</em><br/>
  <em>Building systems that donâ€™t break.</em>
</p>